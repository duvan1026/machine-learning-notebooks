{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='blue'> ANALÍTICA DE BIG DATA <br> Redes de Neuronas Artificiales <br> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> Introducción </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las Redes Neuronales Artificiales (RNA) son un conjunto de modelos matemáticos, en general, de carácter no lineal basados en el funcionamiento del cerebro humano. El primer modelo nació en 1943, propuesto por McCulloch y Pitts y en él, únicamente se podían utilizar neuronas con valores binarios, ya fuese 0,1 ó -1,1. Más adelante, surgieron nuevos hitos dentro del campo de las redes neuronales, como fue la propuesta en 1958 del Perceptrón por parte de Rosenblatt, luego hubo un tiempo en el que quedaron en el olvido para volver a resurgir en los años 80 del siglo XX.\n",
    "\n",
    "   \n",
    "* ¿Qué aplicaciones tiene?\n",
    "    * Problemas de clasificación y predicción\n",
    "    * Aproximación a funciones no lineales\n",
    "    * Optimización\n",
    "    * Control de proceso\n",
    "    \n",
    "* ¿Qué propiedades tienen?\n",
    "    * Aprendizaje adaptativo\n",
    "    * Autoorganización\n",
    "    * Tolerancia a fallos parciales\n",
    "    * Operación en tiempo real\n",
    "    * Fácil inserción dentro de la tecnología existente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> Origen biológico  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas redes, simulan el funcionamiento del cerebro humano y sus conexiones sinápticas, tanto que se podría decirse que las RNA son un modelo, a pequeña escala, del cerebro humano en todas las vertientes. La principal célula del sistema nervioso es la neurona, que equivaldría a la principal unidad de procesamiento de las RNA, la neurona artificial. Del mismo modo, las neuronas transmiten información mediante una diferencia de potencial, mientras que las neuronas de las RNA, la transmite mediante funciones de activación. Ocurre de igual modo, con la forma en la se transmite la información. Las neuronas reciben el estímulo a través de las dendritas, se procesa la información y luego se propaga mediante el axón. Mientras que, en el caso de la neurona artificial, se recibe la información a través de una entrada, se procesa mediante una función de transferencia y se propaga a través de la función de activación. Estas simulitudes entre el modelo biológico y el artificial, se pueden observar en la siguiente figura, que ha sido recogida del curso de la Universidad de Stanford y adaptada al español:\n",
    "\n",
    "<img src=\"../Figuras/SimilitudNeurona.png\" alt=\"SimilitudNeurona\" style=\"width:800px;\"/>\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un alto nivel de abstracción su funcionamiento es sencillo. Las entradas a la neurona artificial son el estímulo que recibe del entorno que la rodea, y la salida es la respuesta a tal estímulo:\n",
    "\n",
    "\n",
    "<img src=\"../Figuras/NeuronaArtificial.png\" alt=\"Neurona\" width=\"600\"/>\n",
    "\n",
    "Los $n$ valores del conjunto de datos serán las **entradas** $x_{1}, x_{2}, ..., x_{n}$ a la neurona, siendo $n>0$. La conexión de cada una de estas entradas con la neurona es lo que se llama **sinapsis**, y las señales que se reciben por dicha sinapsis están representadas por valores numéricos que reciben el nombre de **pesos sinápticos**,  $w_{1}, w_{2}, ..., w_{n}$. La combinación lineal de los valores de entrada con los pesos es la entrada neta a la neurona $j$: $$net_j=\\sum_{i=1}^n w_{ji} \\cdot x_i - \\theta_j$$\n",
    "siendo $\\theta_j$ el término indepediente (**sesgo o bias**), un caso especial de coeficiente sináptico que actúa como el valor umbral (de activación), es decir, el valor que debe ser superado por la suma ponderada de las entradas para provocar la activación de la neurona, permitiendo así generar una salida.\n",
    "\n",
    "La salida viene dada, por tanto, por la expresión:\n",
    "\n",
    "$$y_j = f(net_j) = f(\\sum_{i=1}^n w_{ji} \\cdot x_i - \\theta_j)$$\n",
    "\n",
    "Este valor umbral se suele expresar de una forma más compacta, es decir, $− θ_j$ se le representa por $w_j0$. definiendo una nueva variable $x_0$ idénticamente 1, quedando definida la salida como:\n",
    "\n",
    "\n",
    "$$y_j = f(net_j) = f(\\sum_{i=0}^n w_{ji} \\cdot x_i)$$\n",
    "\n",
    "\n",
    "En esta expresión f representa la **función de activación o transferencia** que al aplicarla a la entrada neta produce el estado de excitación o activación de la neurona. Las funciones más utilizadas son sigmoidales, que incluye la logística y tangente hiperbólica:\n",
    "\n",
    "\n",
    "<img src=\"../Figuras/Funciones.png\" alt=\"drawing\" style=\"width:400px;\"/>\n",
    "\n",
    "El proceso de **aprendizaje** de la red consiste en el ajuste de los pesos sinápticos para que el comportamiento de la red sea el esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='blue'> Perceptrón </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basados en el modelos de neurona de McCulloch-Pitss, Rosenblatt presentó en 1957 un modelo de red, llamado Perceptrón, que era capaz de generalizar, es decir, después de aprender de un conjunto de patrones reconocía otros similares. De este modo el perceptrón da una cierta respuesta ante una determinada entrada, entonces devuelve la misma respuesta para una entrada nueva pero similar.\n",
    "\n",
    "Un ejemplo para resolver con esta arquitectura de red neuronal es el problema de la puerta lógica AND. La red tiene entonces dos entradas *x1, x2* y una salida:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "\n",
    "| Entradas | Salida |\n",
    "| ------- | ------ |\n",
    "| [0, 0]  |   0   |\n",
    "| [1, 0]  |   0   |\n",
    "| [0, 1]  |   0   |\n",
    "| [1, 1]  |   1   |\n",
    "\n",
    "</center>\n",
    "\n",
    "El primer paso es importar las bibliotecas que vamos a utilizar, en este caso, Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import gc    \n",
    "import warnings\n",
    "import os\n",
    "from IPython.display import Image\n",
    "import matplotlib.animation as animation\n",
    "import platform\n",
    "\n",
    "basedir =  os.getcwd()\n",
    "system = platform.system()\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "# Estos parámetros ajustan el rango en el que se visualizan las gráficas\n",
    "\n",
    "GRID_X_START = -1\n",
    "GRID_X_END = 2.5\n",
    "GRID_Y_START = -0.5\n",
    "GRID_Y_END = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos ahora la entrada y la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.array([[0,0],[1,0], [0,1], [1,1]])\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.array([[0, 0, 0, 1]]).T\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si visualizamos los datos de entrada, tendremos algo así:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "for i in np.arange(0,4):\n",
    "        if output[i] == -1:\n",
    "            sns.scatterplot(x=pd.Series(input[i,0]), y=pd.Series(input[i,1]),   marker='P', s = 115, color='r')\n",
    "        else:\n",
    "            sns.scatterplot(x=pd.Series(input[i,0]), y=pd.Series(input[i,1]), marker='X', s=115, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien, ya tenemos otros dos componentes de las redes neuronales. Para el resto, podemos construir una pequeña clase en la que contemplemos los componentes restantes, como los pesos y las funciones de transferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "\n",
    "sns.set_style('dark')\n",
    "# Extraído de https://github.com/SkalskiP/ILearnDeepLearning.py y adaptado para Perceptrón y MLP\n",
    "def plot_decision_regions(X, y, modelo, title, resolution=0.02, ismlp=False, keras=None):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    axes = plt.gca()\n",
    "    axes.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "    plt.title(title, fontsize=16)\n",
    "    grid = np.mgrid[GRID_X_START:GRID_X_END:100j,GRID_X_START:GRID_Y_END:100j]\n",
    "    grid_2d = grid.reshape(2, -1).T\n",
    "    XX, YY = grid\n",
    "    if ismlp==True and keras == None:\n",
    "        _, Z = modelo.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "        Z = Z.reshape(XX.shape)\n",
    "    if ismlp==False and keras == None: \n",
    "        Z = modelo.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "        Z = Z.reshape(XX.shape)\n",
    "    if keras != None:\n",
    "        Z = modelo.predict(grid_2d, batch_size=32, verbose=0)\n",
    "    plt.contourf(XX, YY, Z.reshape(XX.shape), 25, alpha = 0.9, cmap=cm.Spectral)\n",
    "    plt.xlim(XX.min(), YY.max())\n",
    "    plt.ylim(XX.min(), YY.max())\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=60, cmap=plt.cm.Spectral, edgecolors='black')\n",
    "    if not os.path.exists('./visualizacion'):\n",
    "        os.makedirs('./visualizacion')\n",
    "    plt.savefig('./visualizacion/'+title+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraído de: Python Machine Learning de Sebastian Raschka\n",
    "\n",
    "class RNA():\n",
    "    \n",
    "    #En primer lugar la inicialización de la red\n",
    "    def __init__(self, ratio_aprend, n_epoch, random_state=1):\n",
    "        #Los pesos están en un rango [-1,1]\n",
    "        self.ratio_aprend = ratio_aprend\n",
    "        self.n_epoch = n_epoch\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def predict(self, entrada):\n",
    "        entrada_red = np.dot(entrada, self.pesos[1:]) +self.pesos[0]\n",
    "        return np.tanh(entrada_red)\n",
    "        \n",
    "    #El entrenamiento se realiza con una entrada, una salida y un \n",
    "    #número de iteraciones, que en las RNA se llaman épocas.\n",
    "    def train(self, entrada, salida):\n",
    "        \n",
    "        #Creamos una semilla para los números aleatorios.\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        \n",
    "        #Se añade un peso más, que se utilizará como bias.\n",
    "        self.pesos = rgen.normal(loc=0.0, scale=0.01, size=entrada.shape[1] + 1)\n",
    "        self.errores = []\n",
    "        \n",
    "        for iter in range(self.n_epoch):\n",
    "            errors = 0            \n",
    "            for xi,y in zip(entrada, salida):\n",
    "                salida_red = self.predict(xi)\n",
    "                error = y - salida_red\n",
    "                ajuste = self.ratio_aprend * error\n",
    "                self.pesos[1:] += ajuste * xi\n",
    "                self.pesos[0] += ajuste\n",
    "                errors += (error**2).mean()\n",
    "                \n",
    "            self.errores.append(errors)\n",
    "            plot_title = \"Perceptrón {:03}\".format(iter)\n",
    "            plot_decision_regions(entrada,salida, self, title=plot_title, ismlp=False,)\n",
    "        return self    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "red = RNA(0.01, 100, 1)\n",
    "rna = red.train(input,output)\n",
    "#Es posible acceder a los errores cometidos y visualizarlos.\n",
    "'''\n",
    "La salida se corresponderá con la clase más cercana, por tanto, se redondea la salida obtenida,\n",
    "ya que se trate de un problema de clasificación y no de regresión.\n",
    "'''\n",
    "print(round(red.predict([1,0.99])))\n",
    "\n",
    "#sns.lineplot(x=pd.Series(np.arange(1,n_epoch)),y=pd.Series(rna.errores))\n",
    "plt.plot(range(n_epoch),rna.errores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install imagemagick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder visualizar la división entre las clases es necesario tener instalado ImageMagick, así se podrá ver de forma visual el proceso del gradiente descendente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!magick -delay 10 -loop 0 ./visualizacion/Perceptrón*.png Perceptron.gif\n",
    "dir = basedir+\"/visualizacion\"\n",
    "if system == 'Windows':\n",
    "    !rd /s /q \"$dir\"\n",
    "else:\n",
    "    !rm -rf \"$dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!convert -delay 10 -loop 0 ./visualizacion/Perceptrón*.png Perceptron.gif\n",
    "dir = basedir+\"/visualizacion\"\n",
    "if system == 'Windows':\n",
    "    !rd /s /q \"$dir\"\n",
    "else:\n",
    "    !rm -rf \"$dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"Perceptron.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, cambie las entradas y las salidas para que se correspondan con las del problema de la puerta lógica OR, visualizando en el proceso los puntos que corresponden a cada categoría. Además, pruebe con distintos valores de ratio de aprendizaje para comprobar los cambios que se producen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es lo que observa? ¿Se realiza la separación de forma correcta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habrá podido comprobar que no se realiza la separación de forma correcta. Esto es debido a que el problema del XOR no es linealmente separable, es decir, no es posible trazar una única línea que separe los conjuntos de datos de cada categoría. La solución para este tipo de problemas es añadir capas intermedias entre la capa de entrada y la capa de salida del Perceptrón. Estas capas intermedias, llamadas ocultas, permiten resolver problemas linealmente no separables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Perceptrón Multicapa </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones no separables linealmente no pueden ser representadas con un perceptrón simple. Una solución a esta limitación del percetrón simple es el perceptrón multicapa, se añaden capas ocultas. \n",
    "\n",
    "El uso de las capas ocultas junto con funciones de activación como la sigmoide, permite resolver el problema del xor. Por tanto, tendremos una arquitectura como la siguiente:\n",
    "\n",
    "<img src=\"../Figuras/MLP.png\" alt=\"drawing\" style=\"width:400px;\"/>\n",
    "\n",
    "Tendremos que realizar variaciones en la clase que habíamos introducido antes. En primer lugar, se va a contemplar la función sigmoide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraído de Python Machine Learning de Sebastian Raschka\n",
    "import os\n",
    "\n",
    "class Capa:\n",
    "    def __init__(self,entradas, salidas):\n",
    "        rgen = np.random.RandomState(1)\n",
    "        self.pesos = 2*np.random.random((salidas, entradas))-1\n",
    "\n",
    "class RNA2():\n",
    "    #En primer lugar la inicialización de la red\n",
    "    def __init__(self,capa1,capa2,capa3, ratio_aprend, n_epoch, random_state=1):\n",
    "        #Los pesos están en un rango [-1,1]\n",
    "        self.capa1 = capa1\n",
    "        self.capa2 = capa2\n",
    "        self.capa3 = capa3\n",
    "        self.ratio_aprend = ratio_aprend\n",
    "        self.n_epoch = n_epoch\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def tanh(self, entrada, derivada=False):\n",
    "        if derivada==True:\n",
    "            return 1.0 - np.tanh(entrada)**2\n",
    "        return np.tanh(entrada)\n",
    "    \n",
    "    def predict(self, entrada):\n",
    "        salida_capa1 = self.tanh(np.dot(entrada, self.capa1.pesos))\n",
    "        salida_capa2 = self.tanh(np.dot(salida_capa1, self.capa2.pesos))\n",
    "        salida_capa3 = self.tanh(np.dot(salida_capa2, self.capa3.pesos))\n",
    "        return salida_capa2, salida_capa3\n",
    "        \n",
    "    #El entrenamiento se realiza con una entrada, una salida y un \n",
    "    #número de iteraciones, que en las RNA se llaman épocas.\n",
    "    def train(self, entrada, salida):\n",
    "        self.errores = []\n",
    "        for iter in range(self.n_epoch):\n",
    "            plot_title = \"MLP {:03}\".format(iter)\n",
    "            errors = 0\n",
    "            \n",
    "            salida_capa1 =  self.tanh(np.dot(entrada, self.capa1.pesos))\n",
    "            salida_capa2 =  self.tanh(np.dot(salida_capa1, self.capa2.pesos))\n",
    "            salida_capa3 = self.tanh(np.dot(salida_capa2, self.capa3.pesos))\n",
    "            \n",
    "            error_capa3 = salida - salida_capa3\n",
    "            delta_capa3 = error_capa3 * self.tanh(salida_capa3, True)\n",
    "\n",
    "            error_capa2 = delta_capa3.dot(self.capa3.pesos.T)\n",
    "            delta_capa2 = error_capa2 * self.tanh(salida_capa2, True)\n",
    "\n",
    "            \n",
    "            error_capa1 = delta_capa2.dot(self.capa2.pesos.T)\n",
    "            delta_capa1 = error_capa1 * self.tanh(salida_capa1, True)\n",
    "                \n",
    "            ajuste_capa1 = self.ratio_aprend * entrada.T.dot(error_capa1)\n",
    "            ajuste_capa2 = self.ratio_aprend * salida_capa1.T.dot(error_capa2)\n",
    "            ajuste_capa3 = self.ratio_aprend * salida_capa2.T.dot(error_capa3)\n",
    "    \n",
    "            self.capa1.pesos += ajuste_capa1\n",
    "            self.capa2.pesos += ajuste_capa2 \n",
    "            self.capa3.pesos += ajuste_capa3\n",
    "            \n",
    "            errors += error_capa3.mean()**2\n",
    "            \n",
    "            plot_decision_regions(entrada,salida, red, plot_title, 0.02, True, None)\n",
    "            self.errores.append(errors)\n",
    "\n",
    "        return self    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "capa1 = Capa(6,2)\n",
    "capa2 = Capa(4,6)\n",
    "capa3 = Capa(1,4)\n",
    "\n",
    "input = np.array([[0,0],[0,1], [1,0], [1,1]])\n",
    "output = np.array([[1, -1, -1, 1]]).T\n",
    "\n",
    "n_epoch = 250\n",
    "red = RNA2(capa1,capa2, capa3, 0.007, n_epoch, 0)\n",
    "rna = red.train(input,output)\n",
    "_, salida = red.predict(np.array([0.5,0.5]))\n",
    "print(salida)\n",
    "sns.lineplot(x=pd.Series(np.arange(1,n_epoch)),y=pd.Series(rna.errores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!magick -delay 10 -loop 0 ./visualizacion/mlp*.png mlp.gif\n",
    "dir = basedir+\"/visualizacion\"\n",
    "if system == 'Windows':\n",
    "    !rd /s /q \"$dir\"\n",
    "else:\n",
    "    !rm -rf \"$dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"mlp.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya es sabido, Scikit es una biblioteca que incluye multitud de algoritmos de aprendizaje automático. Entre ellos, el Perceptrón. Es posible llevar a cabo todas las operaciones que hemos realizado antes mediante el uso sólo unas pocas funciones de esta biblioteca, al igual que otros modelos estudiados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn = Perceptron(max_iter=40, eta0=0.1, tol=0.19, random_state=0)\n",
    "ppn.fit(input,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida_red = ppn.predict([[1,.8]])\n",
    "salida_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual modo, compruebe qué ocurre con los problemas del OR y del XOR y compruebe si se corresponden los resultados con los obtenidos antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(3), alpha=0.01, activation='tanh', max_iter=2500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.array([[0,0],[0,1], [1,0], [1,1]])\n",
    "output = np.array([[1, -1, -1, 1]]).T\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(input,output.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict([[1,0.9]])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mlp.loss_curve_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que Scikit, Keras es una biblioteca que permite utilizar numerosos algoritmos de aprendizaje automático. En estos últimos años ha ido ganando popularidad gracias a su coexistencia con Tensorflow y la actualidad de temas como el Aprendizaje Profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para la creación de redes \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#Dense nos permite personalizar la arquitectura de cada capa\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#Gradiente descendiente\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.array([[0,0],[0,1], [1,0], [1,1]])\n",
    "output = np.array([[1, -1, -1, 1]]).T\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "def callback_keras_plot(epoch, logs):\n",
    "    plot_title = \"Keras {:03}\".format(epoch)\n",
    "    plot_decision_regions(input, output, modelo=mlp, title = plot_title, keras=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodelcb = keras.callbacks.LambdaCallback(on_epoch_end=callback_keras_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Para crear una red neuronal con Keras se hace una llamada a la función ``Sequential()``. Tras esto, se irán añadiendo capas a la red con la función ``Dense()``, cuyos parámetros son:\n",
    "- units: Nº de neuronas en la capa\n",
    "- input_dim: Tamaño de la entrada\n",
    "- activation: Función de activación\n",
    "\n",
    "La primera capa debe incluir el tamaño de la entrada a través de input_dim, mientras que en el resto de capas no es obligatorio, se infiere de forma automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Dense(6, input_dim=2,activation='linear'))\n",
    "mlp.add(Dense(22, activation='tanh'))\n",
    "mlp.add(Dense(8, activation='relu'))\n",
    "mlp.add(Dense(14, activation='sigmoid'))\n",
    "mlp.add(Dense(4, activation='linear'))\n",
    "mlp.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tengamos definido el diseño de la red únicamente queda añadir cómo medir el error y qué método seguir para alcanzar el mínimo error. Utilizaremos el MSE y el método del gradiente descendente en los parámetros de `loss` y `optimizer` respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(learning_rate=0.3)\n",
    "mlp.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "mlp.fit(input, output, epochs=250, verbose=0, callbacks=[testmodelcb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!magick -delay 10 -loop 0 ./visualizacion/keras*.png keras.gif\n",
    "dir = basedir+\"/visualizacion\"\n",
    "if system == 'Windows':\n",
    "    !rd /s /q \"$dir\"\n",
    "else:\n",
    "    !rm -rf \"$dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"keras.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Conclusión </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha construido el modelo de perceptrón simple y el modelo de perceptrón multicapa, de manera manual, y se ha visualizado la clasificación. Posteriormente, se ha comprobado la facilidad que proporciona, para la construcción de ambos modelos, las bibliotecas Scikit-learn y Keras. \n",
    "\n",
    "Las redes neuronales artificiales son modelos muy poderosos con los que se pueden realizar diversas tareas: clasificación y regresión (aprendizaje supervisado); clustering (SOM, Redes de Kohonen) y otras tareas de aprendizaje no supervisado (Autoencoders, GAN,...); procesamiento secuencial (RNN, LSTM, GRU); aprendizaje con refuerzo (Deep Q-learning, Policy Gradient), etc. \n",
    "\n",
    "Pueden capturar relaciones complejas entre las variables y funcionan bien con grandes cantidades de datos, especialmente en presencia de redundancia o ruido, siempre que estén bien diseñadas. Pueden ser adaptadas a problemas específicos mediante arquitecturas personalizadas, como CNN (redes convolucionales) para imágenes o RNN (redes recurrentes) para datos secuenciales.\n",
    "\n",
    "Para obtener buenos resultados, suelen requerir muchos datos etiquetados, especialmente en el caso de aprendizaje supervisado. Suelen ser costosas computacionalmente. Son consideradas como cajas negras, por la dificultad de entender por qué toman ciertas decisiones. Si no están bien regularizadas pueden memorizar los datos de entrenamiento, por lo que no generalizan adecuademente (sobreajuste). Y su rendimiento depende de la selección de los valores de los hiperparámetros, lo cual puede ser un proceso largo y complejo.\n",
    "\n",
    "Pueden ser aplicados para resolución de multitud de problemas: Reconocimiento de imágenes y vídeos, detección de objetos, procesamiento del lenguaje natural, predicción de series temporales, sistemas recomendadores, robótica y juegos (aprendizaje con refuerzo), generación de datos (imágenes, música, videos, texto o simulaciones),..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\"/> \n",
    "\n",
    "Esta obra está bajo una Licencia Creative Commons Atribución-NoComercial-CompartirIgual 4.0 Internacional.\n",
    "Para ver una copia de esta licencia, véase http://creativecommons.org/licenses/by/4.0/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
