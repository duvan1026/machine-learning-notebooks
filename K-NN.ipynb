{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabla de contenidos\n",
    "* [Tabla de contenidos](#Tabla-de-contenidos)\n",
    "* [<font color='blue'> $k$-VECINOS MÁS CERCANOS <br> </font>](#<font-color='blue'>-$k$-VECINOS-MÁS-CERCANOS-<br>-</font>)\n",
    "\t* [<font color='blue'> Introducción </font>](#<font-color='blue'>-Introducción-</font>)\n",
    "\t* [<font color='blue'> Algoritmo k-Nearest Neighbours (k-NN) </font>](#<font-color='blue'>-Algoritmo-k-Nearest-Neighbours-%28k-NN%29-</font>)\n",
    "\t* [<font color='blue'> Pasos del Algoritmo </font>](#<font-color='blue'>-Pasos-del-Algoritmo-</font>)\n",
    "\t* [<font color='blue'> Ejemplo de clasificación con el conjunto de datos Iris </font>](#<font-color='blue'>-Ejemplo-de-clasificación-con-el-conjunto-de-datos-Iris-</font>)\n",
    "\t\t* [Importar las bibliotecas](#Importar-las-bibliotecas)\n",
    "\t\t* [Importar el conjunto de datos](#Importar-el-conjunto-de-datos)\n",
    "\t\t* [Escalar los datos](#Escalar-los-datos)\n",
    "\t\t* [Generar subconjuntos de entrenamiento y test](#Generar-subconjuntos-de-entrenamiento-y-test)\n",
    "\t\t* [Construir el modelo $k$-NN](#Construir-el-modelo-$k$-NN)\n",
    "\t\t* [Evaluar el modelo](#Evaluar-el-modelo)\n",
    "\t\t* [Elegir el mejor valor de $k$](#Elegir-el-mejor-valor-de-$k$)\n",
    "\t\t* [Clasificar nuevos ejemplos](#Clasificar-nuevos-ejemplos)\n",
    "\t* [<font color='blue'> Conclusiones </font>](#<font-color='blue'>-Conclusiones-</font>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> $k$-VECINOS MÁS CERCANOS <br> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Introducción </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se basa en el almacenamiento de ejemplos y, cuando se quiere clasificar un nuevo objeto, se extraen los objetos más parecidos y se usa su clasificación para clasificar al nuevo objeto.\n",
    "* Su mayor diferencia con otros algoritmos se basa en la trivialidad del proceso de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Algoritmo k-Nearest Neighbours (k-NN) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Es sencillo y fácil de implementar\n",
    "* Robusto frente al ruido\n",
    "* Sus funciones de proximidad (funciones que deciden la clasificación):\n",
    "    * Retomamos las funciones de distancia usadas para clustering\n",
    "    * Los atributos numéricos pueden sufrir un proceso de normalización para evitar que atributos con valores altos tengan más peso\n",
    "    * Admite funciones de proximidad que asignen peso a los atributos, lo que permite eliminar atributos irrelevantes, asignar más peso a los ejemplos más cercanos, ...\n",
    "* El resultado de la clasificación será:\n",
    "    * la clase más frecuente de los k-vecinos (clases discretas)\n",
    "    * la media de las clasificaciones (clases continuas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Pasos del Algoritmo </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calcula la distancia entre la observación a clasificar y el resto de ejemplos del conjunto de entrenamiento.\n",
    "1. Se seleccionan los $k$ registros con menor distancia a la observación.\n",
    "1. Si todos pertenecen a la misma clase, dicha observación es clasificada en esa clase.\n",
    "1. Si no todos pertenecen a la misma clase, dicha observación se clasifica a la clase más votada, la más frecuente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar este algoritmo se puede consultar \n",
    "\n",
    "[sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
    "\n",
    "de dónde han sido extraídos los ejemplos que se muestran a lo largo de este cuaderno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra un ejemplo sencillo, considerando el valor de $k$=3 y siendo el conjunto de entrenamiento X y la salida y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "kNN = KNeighborsClassifier(n_neighbors=3)\n",
    "kNN.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se predice la salida para estos ejemplos:\n",
    "print(kNN.predict([[1.1]]))\n",
    "print(kNN.predict_proba([[0.9]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo también se puede usar en aprendizaje no supervisado. Fíjese en el siguiente ejemplo, en este caso _fit_ sólo usa el parámetro de entrada, no recibe la salida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "# indices[:, 1] toma el segundo vecino (el más cercano que no sea el propio punto)\n",
    "indices = indices[:, 1]\n",
    "distances = distances[:, 1]\n",
    "\n",
    "print(\"Vecinos más cercanos (excluyendo el propio punto):\")\n",
    "print(indices)\n",
    "print(\"Distancias:\")\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Ejemplo de clasificación con el conjunto de datos Iris </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar las bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es importar las bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesado y modelado\n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, GridSearchCV \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier  # k-NN para clasificación\n",
    "\n",
    "\n",
    "# Gráficos\n",
    "# ------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "\n",
    "# Configuración warnings\n",
    "# ------------------------------------------------------------------------------\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el conjunto de datos Iris\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables independientes\n",
    "X = iris.data[:, :]\n",
    "# Variable a predecir\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar subconjuntos de entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# Transformar el conjunto de datos\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los valores de las variables están ahora escalados entre 0 y 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el modelo $k$-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijar el valor de la vecindad\n",
    "n_neighbors = 3\n",
    " \n",
    "# Crear objeto k-NN classifer\n",
    "knn = KNeighborsClassifier(n_neighbors, metric='euclidean')\n",
    "\n",
    "# Entrenar el clasificador k-NN\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar la respuesta sobre el mismo conjunto de entrenamiento\n",
    "print('Acc del clasificador k-NN sobre el conjunto de entrenamiento: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmemos la precisión obteniendo la matriz de confusión y _report_ sobre la predicción del conjunto test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir la respuesta sobre el conjunto de prueba\n",
    "\n",
    "# Se puede usar la función Score\n",
    "print('Acc del clasificador k-NN sobre el conjunto test: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O bien algunas métricas como la matriz de confusión, precision, recall,...\n",
    "y_pred = knn.predict(X_test)\n",
    "print('Matriz de confusión: ')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elegir el mejor valor de $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a representar gráficamente los resultados obtenidos con distintos valores de $k$, de manera que nos ayude a seleccionar un valor óptimo para este parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1,10)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "plt.style.use('ggplot')\n",
    "plt.figure()\n",
    "plt.title('Accuracy Rate K Value')\n",
    "plt.xlabel('k value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.scatter(k_range,scores)\n",
    "plt.xticks([0,2,4,6,8,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando la gráfica se puede seleccionar el mejor valor para $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede buscar el mejor valor para $k$ usando la conocida función GridSearchCV. Es una buena solución que permite hacer una búsqueda de los valores de los hiperparámetros en los modelos de _Machine Learning_. \n",
    "\n",
    "GridSearchCV está disponible en scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos automáticamente el mejor valor de $k$ para este conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_neighbors\": range(1, 20)}\n",
    "gridsearch = GridSearchCV(KNeighborsClassifier(), parameters)\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede usarse para todos los parámetros deseados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "...     \"n_neighbors\": range(1, 20),\n",
    "...     \"weights\": [\"uniform\", \"distance\"],\n",
    "... }\n",
    "\n",
    "gridsearch = GridSearchCV(KNeighborsClassifier(), parameters, cv= RepeatedKFold(n_splits = 10,n_repeats=5))\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores hiperparámetros encontrados \n",
    "print(\"Mejores valores de los hiperparámetros:\")\n",
    "print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "\n",
    "accuracy = gridsearch.best_score_ *100\n",
    "print(\"Precisión del conjunto de entrenamiento: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Se aplica al conjunto test\n",
    "y_pred=gridsearch.predict(X_test)\n",
    "test_accuracy=accuracy_score(y_test,y_pred)*100\n",
    "\n",
    "print(\"Precisión del conjunto test: {:.2f}%\".format(test_accuracy))\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificar nuevos ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se alcance unos valores de las métricas que permitan aceptar el modelo como válido y que, por tanto, está listo para su uso, se llevaría a un entorno real y productivo. En esa situación el modelo se enfrentará a datos totalmente desconocidos, los datos almacenados sobre el conjunto test_real, y no se podrá evaluar la respuesta del modelo, salvo las indicaciones que puedan arrojar los expertos en la materia a partir del conocimiento del dominio del problema a resolver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La predicción se puede hacer de dos formas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predice la clase:\n",
    "print(gridsearch.predict([[6, 2, 4.9, 1.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predice la probabilidad de pertenencia a cada clase\n",
    "print(gridsearch.predict_proba([[6, 2, 4.9, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% de probabilidad para la clase 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Conclusiones </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha construido un modelo de clasificación para procesar y clasificar puntos de un conjunto de entrada con el algoritmo $k$-Nearest Neighbour. Como su nombre indica, se evalúan los $k$ vecinos más cercanos para clasificar nuevos puntos. Al ser utilizado como un algoritmo supervisado, se debe disponer de suficientes muestras etiquetadas para poder entrenar el modelo con buenos resultados. Este algoritmo es bastante simple y poderoso, pero puede necesitar mucha memoria y recursos de CPU para procesar un gran conjunto de datos y evaluar nuevos puntos. Esto no lo hace recomendable para conjuntos de datos muy grandes.\n",
    "\n",
    "El algoritmo $k$-NN se usa para encontrar similitudes de documentos y reconocimiento de patrones. También se emplea para desarrollar sistemas de recomendación y para la reducción de la dimensionalidad, así como en las fases del preprocesamiento para la visión artificial, en particular, las tareas de reconocimiento facial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\"/> \n",
    "\n",
    "Esta obra está bajo una Licencia Creative Commons Atribución-NoComercial-CompartirIgual 4.0 Internacional.\n",
    "Para ver una copia de esta licencia, véase http://creativecommons.org/licenses/by/4.0/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
